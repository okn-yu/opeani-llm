{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ae5eb1",
   "metadata": {},
   "source": [
    "# 概要\n",
    "* きれいに要約させてみる\n",
    "* そもそも\"。\"で分割すれば最初からうまく行っていたのではないか？（精度が多少ましになるくらいか）\n",
    "* 公式ドキュメントの通りtree_summarizeが要約最強である\n",
    "* 出力文字数を増やせるかの確認\n",
    "\n",
    "# 参考\n",
    "* 出力サイズを増やすと今度はチャンクが不足する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0bdc17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "382f4e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jupyterでも実行できるのは知らんかった...\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44238731",
   "metadata": {},
   "source": [
    "# PromptHelperとは\n",
    "* トークンの制約に対応するための汎用プロンプトサポートツール\n",
    "\n",
    "# LLM（Large Language Model）\n",
    "* PromptHelperの引数の大部分はLLMに関連する\n",
    "* つまり最初にLLMが何かを理解する必要がある\n",
    "* (そもそも入力されたPromptをparseするのはLLMなのでPromptHelperがLLMを引数にとるのは当然である)\n",
    "* 詳細はここを見よ！\n",
    "* https://gpt-index.readthedocs.io/en/latest/reference/llm_predictor.html\n",
    "\n",
    "# そもそも\n",
    "* LLMPredictorとはLangchainのLLMChainのラッパーであり、LlamdaIndexと統合することができる\n",
    "* LLMChain ↔ LLMPrecitor ↔ LlamdaIndex\n",
    "* Our LLMPredictor is a wrapper around Langchain’s LLMChain that allows easy integration into LlamaIndex.\n",
    "* llmを引数にとり、デフォルトではOpenAI’s text-davinci-003が指定される\n",
    "* max_input_size: int = 3900, num_output: int = 256\n",
    "* デフォルトでは入力は3900トークンで出力は256トークン\n",
    "* つまり長文の回答はできない\n",
    "\n",
    "# 引数のllm\n",
    "* OpenAI以外にいも複数のLLMをサポートしている\n",
    "* OpenAI, Cohere, Hugging Face, etc\n",
    "* Llama_indexは自身でLLMはサポートしておらず、あくまでもlangchainを介して呼び出しを行う？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68f1f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_index import LLMPredictor, GPTSimpleVectorIndex, SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7d9a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAIのAPIのラッパー\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03cdd0e",
   "metadata": {},
   "source": [
    "# 利用例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "674b88fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:root:> [SimpleDirectoryReader] Total files added: 1\n",
      "DEBUG:root:> Adding chunk: 孫悟空少年編\n",
      "地球の人里離れた山奥に住む尻尾の生えた少年・孫悟空はある日、西の都からやって来た...\n",
      "DEBUG:root:> Adding chunk: \n",
      "\n",
      "その後、悟空は旅の途中に知り合った武術の達人・亀仙人の下で、後に親友となるクリリンと共に8...\n",
      "DEBUG:root:> Adding chunk: 悟空は仇を討つため、道中に出会ったヤジロベーや仙猫カリンの協力を得て命を賭して潜在する力を引き...\n",
      "DEBUG:root:> Adding chunk: \n",
      "\n",
      "サイヤ人編\n",
      "ピッコロ（マジュニア）との闘いから約5年後、息子の孫悟飯を儲けて平和な日々を過...\n",
      "DEBUG:root:> Adding chunk: 仲間の協力もあり、何とか辛勝し撤退させるが、多くの仲間を失うとともに、ピッコロの戦死により彼と...\n",
      "DEBUG:root:> Adding chunk: フリーザの持つ圧倒的な力の前にはベジータやピッコロ、悟空すら歯が立たず仲間たちが次々と命を落と...\n",
      "DEBUG:root:> Adding chunk: トランクスと名乗るその青年は、自分は未来からやってきたブルマとベジータの息子であることを悟空に...\n",
      "DEBUG:root:> Adding chunk: 悟空らは天界にある1日で1年の修行が行えるも過酷な環境に晒される「精神と時の部屋」で修行し、強...\n",
      "DEBUG:root:> Adding chunk: 天下一武道会の最中、悟空たちは界王よりもさらに高位の存在である界王神から、恐ろしい力を持つ魔人...\n",
      "DEBUG:root:> Adding chunk: ドラゴンボールの協力もあり、地球・ナメック星・あの世の人々のエネルギーによって作り上げられた超...\n",
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_index import (\n",
    "    GPTSimpleVectorIndex, \n",
    "    GPTSimpleKeywordTableIndex, \n",
    "    GPTListIndex, \n",
    "    SimpleDirectoryReader,\n",
    "    GPTTreeIndex\n",
    ")\n",
    "\n",
    "from llama_index import (\n",
    "    GPTKeywordTableIndex, \n",
    "    SimpleDirectoryReader, \n",
    "    LLMPredictor,\n",
    "    PromptHelper\n",
    ")\n",
    "\n",
    "from gpt_index.indices.knowledge_graph.base import GPTKnowledgeGraphIndex\n",
    "from langchain import OpenAI\n",
    "\n",
    "documents = SimpleDirectoryReader('story/dragonball').load_data()\n",
    "\n",
    "# define prompt helper\n",
    "# set maximum input size\n",
    "max_input_size = 4096\n",
    "# set number of output tokens\n",
    "num_output = 1024*3\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 0\n",
    "# separatorが\"/n/n\"だとチャンクサイズの計算でエラーになるっぽい。\"。\"の場合は動く模様。\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap, separator=\"。\", chunk_size_limit=512)\n",
    "\n",
    "# define LLM\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\", max_tokens=num_output))\n",
    "\n",
    "# build index\n",
    "index = GPTListIndex(documents, prompt_helper=prompt_helper, llm_predictor=llm_predictor)\n",
    "#index = GPTTreeIndex(documents, prompt_helper=prompt_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1f7d2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:root:> Searching in chunk: 孫悟空少年編\n",
      "地球の人里離れた山奥に住む尻尾の生えた少年・孫悟空はある日、西の都からやって来た...\n",
      "DEBUG:root:> Searching in chunk: \n",
      "\n",
      "その後、悟空は旅の途中に知り合った武術の達人・亀仙人の下で、後に親友となるクリリンと共に8...\n",
      "DEBUG:root:> Searching in chunk: 悟空は仇を討つため、道中に出会ったヤジロベーや仙猫カリンの協力を得て命を賭して潜在する力を引き...\n",
      "DEBUG:root:> Searching in chunk: \n",
      "\n",
      "サイヤ人編\n",
      "ピッコロ（マジュニア）との闘いから約5年後、息子の孫悟飯を儲けて平和な日々を過...\n",
      "DEBUG:root:> Searching in chunk: 仲間の協力もあり、何とか辛勝し撤退させるが、多くの仲間を失うとともに、ピッコロの戦死により彼と...\n",
      "DEBUG:root:> Searching in chunk: フリーザの持つ圧倒的な力の前にはベジータやピッコロ、悟空すら歯が立たず仲間たちが次々と命を落と...\n",
      "DEBUG:root:> Searching in chunk: トランクスと名乗るその青年は、自分は未来からやってきたブルマとベジータの息子であることを悟空に...\n",
      "DEBUG:root:> Searching in chunk: 悟空らは天界にある1日で1年の修行が行えるも過酷な環境に晒される「精神と時の部屋」で修行し、強...\n",
      "DEBUG:root:> Searching in chunk: 天下一武道会の最中、悟空たちは界王よりもさらに高位の存在である界王神から、恐ろしい力を持つ魔人...\n",
      "DEBUG:root:> Searching in chunk: ドラゴンボールの協力もあり、地球・ナメック星・あの世の人々のエネルギーによって作り上げられた超...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A single term is larger than the allowed chunk size.\nTerm size: 131\nChunk size: 97Effective chunk size: 97",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m詳しく要約してください\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtree_summarize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/llama_index/indices/base.py:395\u001b[0m, in \u001b[0;36mBaseGPTIndex.query\u001b[0;34m(self, query_str, mode, query_transform, use_async, **query_kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m query_config \u001b[38;5;241m=\u001b[39m QueryConfig(\n\u001b[1;32m    380\u001b[0m     index_struct_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct\u001b[38;5;241m.\u001b[39mget_type(),\n\u001b[1;32m    381\u001b[0m     query_mode\u001b[38;5;241m=\u001b[39mmode_enum,\n\u001b[1;32m    382\u001b[0m     query_kwargs\u001b[38;5;241m=\u001b[39mquery_kwargs,\n\u001b[1;32m    383\u001b[0m )\n\u001b[1;32m    384\u001b[0m query_runner \u001b[38;5;241m=\u001b[39m QueryRunner(\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm_predictor,\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt_helper,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m     use_async\u001b[38;5;241m=\u001b[39muse_async,\n\u001b[1;32m    394\u001b[0m )\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index_struct\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/llama_index/indices/query/query_runner.py:120\u001b[0m, in \u001b[0;36mQueryRunner.query\u001b[0;34m(self, query_str_or_bundle, index_struct)\u001b[0m\n\u001b[1;32m    110\u001b[0m query_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_query_kwargs(config)\n\u001b[1;32m    111\u001b[0m query_obj \u001b[38;5;241m=\u001b[39m query_cls(\n\u001b[1;32m    112\u001b[0m     index_struct,\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mquery_kwargs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m     use_async\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_async,\n\u001b[1;32m    118\u001b[0m )\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/llama_index/token_counter/token_counter.py:55\u001b[0m, in \u001b[0;36mllm_token_counter.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m start_token_ct \u001b[38;5;241m=\u001b[39m llm_predictor\u001b[38;5;241m.\u001b[39mtotal_tokens_used\n\u001b[1;32m     53\u001b[0m start_embed_token_ct \u001b[38;5;241m=\u001b[39m embed_model\u001b[38;5;241m.\u001b[39mtotal_tokens_used\n\u001b[0;32m---> 55\u001b[0m f_return_val \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m net_tokens \u001b[38;5;241m=\u001b[39m llm_predictor\u001b[38;5;241m.\u001b[39mtotal_tokens_used \u001b[38;5;241m-\u001b[39m start_token_ct\n\u001b[1;32m     58\u001b[0m llm_predictor\u001b[38;5;241m.\u001b[39mlast_token_usage \u001b[38;5;241m=\u001b[39m net_tokens\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/llama_index/indices/query/base.py:302\u001b[0m, in \u001b[0;36mBaseGPTIndexQuery.query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;129m@llm_token_counter\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TYPE:\n\u001b[1;32m    301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Answer a query.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;66;03m# if include_summary is True, then include summary text in answer\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;66;03m# summary text is set through `set_text` on the underlying index.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# TODO: refactor response builder to be in the __init__\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_mode \u001b[38;5;241m!=\u001b[39m ResponseMode\u001b[38;5;241m.\u001b[39mNO_TEXT \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_include_summary:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/llama_index/indices/query/base.py:276\u001b[0m, in \u001b[0;36mBaseGPTIndexQuery._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_builder\u001b[38;5;241m.\u001b[39madd_text_chunks([text])\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_mode \u001b[38;5;241m!=\u001b[39m ResponseMode\u001b[38;5;241m.\u001b[39mNO_TEXT:\n\u001b[0;32m--> 276\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_give_response_for_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/llama_index/indices/query/base.py:218\u001b[0m, in \u001b[0;36mBaseGPTIndexQuery._give_response_for_nodes\u001b[0;34m(self, query_str)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_give_response_for_nodes\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_str: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TEXT_TYPE:\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Give response for nodes.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_response_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_response_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/llama_index/indices/response/builder.py:280\u001b[0m, in \u001b[0;36mResponseBuilder.get_response\u001b[0;34m(self, query_str, prev_response, mode, **response_kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_response_compact(query_str, prev_response)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ResponseMode\u001b[38;5;241m.\u001b[39mTREE_SUMMARIZE:\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_response_tree_summarize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/llama_index/indices/response/builder.py:239\u001b[0m, in \u001b[0;36mResponseBuilder._get_response_tree_summarize\u001b[0;34m(self, query_str, prev_response, num_children)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# then get text splitter\u001b[39;00m\n\u001b[1;32m    236\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_helper\u001b[38;5;241m.\u001b[39mget_text_splitter_given_prompt(\n\u001b[1;32m    237\u001b[0m     summary_template, num_children\n\u001b[1;32m    238\u001b[0m )\n\u001b[0;32m--> 239\u001b[0m text_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m all_nodes: Dict[\u001b[38;5;28mint\u001b[39m, Node] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    241\u001b[0m     i: Node(text\u001b[38;5;241m=\u001b[39mt) \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(text_chunks)\n\u001b[1;32m    242\u001b[0m }\n\u001b[1;32m    244\u001b[0m index_builder \u001b[38;5;241m=\u001b[39m GPTTreeIndexBuilder(\n\u001b[1;32m    245\u001b[0m     num_children,\n\u001b[1;32m    246\u001b[0m     summary_template,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m     use_async\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_async,\n\u001b[1;32m    251\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/llama_index/langchain_helpers/text_splitter.py:108\u001b[0m, in \u001b[0;36mTokenTextSplitter.split_text\u001b[0;34m(self, text, extra_info_str)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, extra_info_str: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Split incoming text and return chunks.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     text_splits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_text_with_overlaps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_info_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_info_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text_split\u001b[38;5;241m.\u001b[39mtext_chunk \u001b[38;5;28;01mfor\u001b[39;00m text_split \u001b[38;5;129;01min\u001b[39;00m text_splits]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.11/site-packages/llama_index/langchain_helpers/text_splitter.py:147\u001b[0m, in \u001b[0;36mTokenTextSplitter.split_text_with_overlaps\u001b[0;34m(self, text, extra_info_str)\u001b[0m\n\u001b[1;32m    145\u001b[0m num_cur_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(cur_token)), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_cur_tokens \u001b[38;5;241m>\u001b[39m effective_chunk_size:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA single term is larger than the allowed chunk size.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTerm size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cur_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEffective chunk size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meffective_chunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m     )\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# If adding token to current_doc would exceed the chunk size:\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# 1. First verify with tokenizer that current_doc\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# 1. Update the docs list\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cur_total \u001b[38;5;241m+\u001b[39m num_cur_tokens \u001b[38;5;241m>\u001b[39m effective_chunk_size:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# NOTE: since we use a proxy for counting tokens, we want to\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# run tokenizer across all of current_doc first. If\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# the chunk is too big, then we will reduce text in pieces\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: A single term is larger than the allowed chunk size.\nTerm size: 131\nChunk size: 97Effective chunk size: 97"
     ]
    }
   ],
   "source": [
    "summary = index.query(\"詳しく要約してください\", response_mode=\"tree_summarize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96762dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fadaf4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6443ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4675d7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
